{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1_uAYf_ERjv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "import tempfile\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwE4y-QnDNLL",
        "outputId": "b4cf6aff-9e36-4740-f4a0-573699a3d09b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 27.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.7)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.21.6)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-efficientnets\n",
            "  Downloading keras_efficientnets-0.1.7-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras-efficientnets) (1.7.3)\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from keras-efficientnets) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.7/dist-packages (from keras-efficientnets) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->keras-efficientnets) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->keras-efficientnets) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->keras-efficientnets) (3.1.0)\n",
            "Installing collected packages: keras-efficientnets\n",
            "Successfully installed keras-efficientnets-0.1.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras.utils\n",
            "  Downloading keras-utils-1.0.13.tar.gz (2.4 kB)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras.utils) (2.8.0)\n",
            "Building wheels for collected packages: keras.utils\n",
            "  Building wheel for keras.utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras.utils: filename=keras_utils-1.0.13-py3-none-any.whl size=2656 sha256=51b9e5b2d92884c38dd9ffd4908818ba45b51452d99deccb7cbf32736ea3eb45\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/dd/3b/493952a5240d486a83805d65360dedadbadeae71d25e2c877f\n",
            "Successfully built keras.utils\n",
            "Installing collected packages: keras.utils\n",
            "Successfully installed keras.utils-1.0.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_applications\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.5.2)\n",
            "Installing collected packages: keras-applications\n",
            "Successfully installed keras-applications-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_model_optimization\n",
        "!pip install keras-efficientnets\n",
        "!pip install keras.utils\n",
        "!pip install keras_applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yitbxR4VdvRl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "#from tqdm import tqdm\n",
        "# tqdm doesn't work well in colab.\n",
        "# This is the solution:\n",
        "# https://stackoverflow.com/questions/41707229/tqdm-printing-to-newline\n",
        "import tqdm.notebook as tq\n",
        "#for i in tq.tqdm(...):\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Hr-X89CugXdN"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoD9n5lcd2VN"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "os.listdir('/content/drive/MyDrive/cifar-100-python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrbkoxX6fBqv"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzCNyg1mfGn1"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "path = '/content/drive/MyDrive/cifar-100-python/train'\n",
        "\n",
        "train_dict = unpickle(path)\n",
        "\n",
        "train_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foD0Z6EWfNnu"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "path = '/content/drive/MyDrive/cifar-100-python/meta'\n",
        "\n",
        "names_dict = unpickle(path)\n",
        "\n",
        "names_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM5tdsSDfRCG"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "fine_labels_list = train_dict[b'fine_labels']\n",
        "coarse_labels_list = train_dict[b'coarse_labels']\n",
        "\n",
        "fine_label_names_list = names_dict[b'fine_label_names']\n",
        "coarse_label_names_list = names_dict[b'coarse_label_names']\n",
        "\n",
        "print(len(fine_labels_list))\n",
        "print(len(coarse_labels_list))\n",
        "print(len(fine_label_names_list))\n",
        "print(len(coarse_label_names_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_Dhd0DYfVTe"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Create the df_train dataframe\n",
        "df_train = pd.DataFrame(fine_labels_list, columns=['fine_labels'])\n",
        "\n",
        "# Create new columns\n",
        "df_train['coarse_labels'] = coarse_labels_list\n",
        "df_train['image_num'] = df_train.index + 100000\n",
        "\n",
        "# Create the image_id column\n",
        "def create_imageid(row):\n",
        "    \n",
        "    image_id = str(row['fine_labels']) + '_' + str(row['coarse_labels']) + '_' + str(row['image_num']) +'.jpg'\n",
        "    \n",
        "    return image_id\n",
        "\n",
        "df_train['image_id'] = df_train.apply(create_imageid, axis=1)\n",
        "\n",
        "\n",
        "# Create the fine and coarse label names columns\n",
        "\n",
        "def create_finelabelname(x):\n",
        "    \n",
        "    # this returns bytes: b'apple'\n",
        "    name = fine_label_names_list[x]\n",
        "    \n",
        "    # convert bytes to string: 'apple'\n",
        "    name = name.decode(\"utf-8\") \n",
        "    \n",
        "    return name\n",
        "\n",
        "\n",
        "def create_coarselabelname(x):\n",
        "    \n",
        "    # this returns bytes: b'apple'\n",
        "    name = coarse_label_names_list[x]\n",
        "    \n",
        "    # convert bytes to string: 'apple'\n",
        "    name = name.decode(\"utf-8\") \n",
        "    \n",
        "    return name\n",
        "\n",
        "\n",
        "df_train['fine_label_names'] = df_train['fine_labels'].apply(create_finelabelname)\n",
        "\n",
        "df_train['coarse_label_names'] = df_train['coarse_labels'].apply(create_coarselabelname)\n",
        "\n",
        "\n",
        "# Remove unnecessary columns\n",
        "df_train = df_train.drop('image_num', axis=1)\n",
        "\n",
        "# Reorder the columns\n",
        "cols = ['image_id', 'fine_label_names', 'fine_labels', 'coarse_label_names', 'coarse_labels']\n",
        "df_train = df_train[cols]\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiyKv6OgfWFu"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "path = '/content/drive/MyDrive/cifar-100-python/test'\n",
        "\n",
        "test_dict = unpickle(path)\n",
        "\n",
        "test_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Y-1N9VfYs2"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "fine_labels_list = test_dict[b'fine_labels']\n",
        "coarse_labels_list = test_dict[b'coarse_labels']\n",
        "\n",
        "fine_label_names_list = names_dict[b'fine_label_names']\n",
        "coarse_label_names_list = names_dict[b'coarse_label_names']\n",
        "\n",
        "print(len(fine_labels_list))\n",
        "print(len(coarse_labels_list))\n",
        "print(len(fine_label_names_list))\n",
        "print(len(coarse_label_names_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_Vi0SgmfbRN"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Create the df_test dataframe\n",
        "df_test = pd.DataFrame(fine_labels_list, columns=['fine_labels'])\n",
        "\n",
        "# Create new columns\n",
        "df_test['coarse_labels'] = coarse_labels_list\n",
        "df_test['image_num'] = df_test.index + 200000\n",
        "\n",
        "# Create the image_id column\n",
        "def create_imageid(row):\n",
        "    \n",
        "    image_id = str(row['fine_labels']) + '_' + str(row['coarse_labels']) + '_' + str(row['image_num']) +'.jpg'\n",
        "    \n",
        "    return image_id\n",
        "\n",
        "df_test['image_id'] = df_test.apply(create_imageid, axis=1)\n",
        "\n",
        "\n",
        "# Create the fine and coarse label names columns\n",
        "\n",
        "def create_finelabelname(x):\n",
        "    \n",
        "    # this returns bytes: b'apple'\n",
        "    name = fine_label_names_list[x]\n",
        "    \n",
        "    # convert bytes to string: 'apple'\n",
        "    name = name.decode(\"utf-8\") \n",
        "    \n",
        "    return name\n",
        "\n",
        "\n",
        "def create_coarselabelname(x):\n",
        "    \n",
        "    # this returns bytes: b'apple'\n",
        "    name = coarse_label_names_list[x]\n",
        "    \n",
        "    # convert bytes to string: 'apple'\n",
        "    name = name.decode(\"utf-8\") \n",
        "    \n",
        "    return name\n",
        "\n",
        "\n",
        "df_test['fine_label_names'] = df_test['fine_labels'].apply(create_finelabelname)\n",
        "\n",
        "df_test['coarse_label_names'] = df_test['coarse_labels'].apply(create_coarselabelname)\n",
        "\n",
        "\n",
        "# Remove unnecessary columns\n",
        "df_test = df_test.drop('image_num', axis=1)\n",
        "\n",
        "# Reorder the columns\n",
        "cols = ['image_id', 'fine_label_names', 'fine_labels', 'coarse_label_names', 'coarse_labels']\n",
        "df_test = df_test[cols]\n",
        "\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWI8Q8Uiffxe"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "df_train.to_csv('train.csv', index=False)\n",
        "df_test.to_csv('test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFYpszUKMhkB"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "train_csv = pd.read_csv('/content/train.csv')\n",
        "test_csv = pd.read_csv('/content/test.csv')\n",
        "\n",
        "train_data = pd.DataFrame(train_csv)\n",
        "test_data =  pd.DataFrame(test_csv)\n",
        "\n",
        "#CLEAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLhniyaEwSL4"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "path_train = '/content/drive/MyDrive/cifar-100-python/train'\n",
        "\n",
        "train_dict = unpickle(path_train)\n",
        "train_matrix = train_dict[b'data']\n",
        "\n",
        "path_test = '/content/drive/MyDrive/cifar-100-python/test'\n",
        "\n",
        "test_dict = unpickle(path_test)\n",
        "test_matrix = test_dict[b'data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i6PTRWZBEKm"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "print(test_matrix.shape)\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-5NGhPgxvJX"
      },
      "outputs": [],
      "source": [
        "# Create people directory\n",
        "if os.path.isdir('people_images') == True:\n",
        "  dir = '/content/people_images'\n",
        "  for f in os.listdir(dir):\n",
        "      os.remove(os.path.join(dir, f))\n",
        "\n",
        "if os.path.isdir('people_images') == False:\n",
        "    train_images = 'people_images'\n",
        "    os.mkdir(train_images)\n",
        "\n",
        "\n",
        "# Create not_people directory\n",
        "if os.path.isdir('not_people_images') == True:\n",
        "  dir = '/content/not_people_images'\n",
        "  for f in os.listdir(dir):\n",
        "      os.remove(os.path.join(dir, f))\n",
        "\n",
        "else:\n",
        "    train_images = 'not_people_images'\n",
        "    os.mkdir(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jny4UK-aNy9a"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "#Combined dataframe of both test and train\n",
        "combined_data = train_data.append(test_data)\n",
        "combined_matrix = np.append(train_matrix,test_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iL_0hYLk88H"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "random = combined_data[(combined_data['fine_labels']==0)].sample(31,replace=False)        # Initialize random dataframe\n",
        "# select 31 random images from each class(fine label)\n",
        "for i in range(1,100):\n",
        "  label_index = combined_data[(combined_data['fine_labels']==i)].sample(31,replace=False)\n",
        "  random = random.append(label_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt3LIbZ2uvbM"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "# People dataframe from combined\n",
        "people_data = combined_data[(combined_data['coarse_label_names']=='people')]\n",
        "\n",
        "# Randomize people dataframe\n",
        "people = people_data.sample(frac=1)\n",
        "people.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCdJViCwlBf9"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "# Remove all the people elements \n",
        "not_people = random[random.coarse_label_names != 'people']\n",
        "\n",
        "# add 55 random elements from not people to make 3000 images (equal to people)\n",
        "not_people = not_people.append(not_people.sample(n=55))\n",
        "\n",
        "#Randomize not_people\n",
        "not_people = not_people.sample(frac=1)\n",
        "\n",
        "#Convert and store not_people dataframe to csv\n",
        "# not_people.to_csv('not_people.csv')\n",
        "print(len(not_people))\n",
        "not_people.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLyOcGDe1nsl"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "count = 0\n",
        "print(len(people))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgxj7vdcxo-B"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "# Prepare train images\n",
        "for i in range(0, train_matrix.shape[0]):\n",
        "    if(train_data.loc[i,'image_id'] in people.values):\n",
        "      \n",
        "      # Get the image_id from the df_train dataframe\n",
        "      image_id = train_data.loc[i, 'image_id']\n",
        "\n",
        "\n",
        "      # Select an image\n",
        "      row = train_matrix[i]\n",
        "\n",
        "      # Extract each channel\n",
        "      ch0 = row[0:1024] \n",
        "      ch1 = row[1024:2048]\n",
        "      ch2 = row[2048:]\n",
        "\n",
        "      # Reshape to 32x32\n",
        "      ch0 = np.reshape(ch0, (32,32)) # red\n",
        "      ch1 = np.reshape(ch1, (32,32)) # green\n",
        "      ch2 = np.reshape(ch2, (32,32)) # blue\n",
        "\n",
        "      # Stack the matrices along the channel axis\n",
        "      image = np.dstack((ch0, ch1, ch2))\n",
        "\n",
        "      \n",
        "      # Save the image in the folder\n",
        "      # that we created.\n",
        "      fname = image_id\n",
        "      dst = os.path.join('people_images', fname)\n",
        "      \n",
        "      # If cv2.COLOR_RGB2BGR is not used then the saved images appear blue.\n",
        "      cv2.imwrite(dst, image)\n",
        "      count = count + 1\n",
        "\n",
        "# Not-People\n",
        "    if(train_data.loc[i,'image_id'] in not_people.values):\n",
        "      # Get the image_id from the df_train dataframe\n",
        "      image_id = train_data.loc[i, 'image_id']\n",
        "\n",
        "\n",
        "      # Select an image\n",
        "      row = train_matrix[i]\n",
        "\n",
        "      # Extract each channel\n",
        "      ch0 = row[0:1024] \n",
        "      ch1 = row[1024:2048]\n",
        "      ch2 = row[2048:]\n",
        "\n",
        "      # Reshape to 32x32\n",
        "      ch0 = np.reshape(ch0, (32,32)) # red\n",
        "      ch1 = np.reshape(ch1, (32,32)) # green\n",
        "      ch2 = np.reshape(ch2, (32,32)) # blue\n",
        "\n",
        "      # Stack the matrices along the channel axis\n",
        "      image = np.dstack((ch0, ch1, ch2))\n",
        "\n",
        "      \n",
        "      # Save the image in the folder\n",
        "      # that we created.\n",
        "      fname = image_id\n",
        "      dst = os.path.join('not_people_images', fname)\n",
        "      \n",
        "      # If cv2.COLOR_RGB2BGR is not used then the saved images appear blue.\n",
        "      cv2.imwrite(dst, image)\n",
        "      count = count + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRA7ST78CVop"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "# Prepare test images\n",
        "for i in range(0, test_matrix.shape[0]):\n",
        "    if(test_data.loc[i,'image_id'] in people.values):\n",
        "      \n",
        "      # Get the image_id from the df_train dataframe\n",
        "      image_id = test_data.loc[i, 'image_id']\n",
        "\n",
        "\n",
        "      # Select an image\n",
        "      row = test_matrix[i]\n",
        "\n",
        "      # Extract each channel\n",
        "      ch0 = row[0:1024] \n",
        "      ch1 = row[1024:2048]\n",
        "      ch2 = row[2048:]\n",
        "\n",
        "      # Reshape to 32x32\n",
        "      ch0 = np.reshape(ch0, (32,32)) # red\n",
        "      ch1 = np.reshape(ch1, (32,32)) # green\n",
        "      ch2 = np.reshape(ch2, (32,32)) # blue\n",
        "\n",
        "      # Stack the matrices along the channel axis\n",
        "      image = np.dstack((ch0, ch1, ch2))\n",
        "\n",
        "      \n",
        "      # Save the image in the folder\n",
        "      # that we created.\n",
        "      fname = image_id\n",
        "      dst = os.path.join('people_images', fname)\n",
        "      \n",
        "      # If cv2.COLOR_RGB2BGR is not used then the saved images appear blue.\n",
        "      cv2.imwrite(dst, image)\n",
        "      count = count + 1\n",
        "\n",
        "# Not-People\n",
        "    if(test_data.loc[i,'image_id'] in not_people.values):\n",
        "      # Get the image_id from the df_train dataframe\n",
        "      image_id = test_data.loc[i, 'image_id']\n",
        "\n",
        "\n",
        "      # Select an image\n",
        "      row = test_matrix[i]\n",
        "\n",
        "      # Extract each channel\n",
        "      ch0 = row[0:1024] \n",
        "      ch1 = row[1024:2048]\n",
        "      ch2 = row[2048:]\n",
        "\n",
        "      # Reshape to 32x32\n",
        "      ch0 = np.reshape(ch0, (32,32)) # red\n",
        "      ch1 = np.reshape(ch1, (32,32)) # green\n",
        "      ch2 = np.reshape(ch2, (32,32)) # blue\n",
        "\n",
        "      # Stack the matrices along the channel axis\n",
        "      image = np.dstack((ch0, ch1, ch2))\n",
        "\n",
        "      \n",
        "      # Save the image in the folder\n",
        "      # that we created.\n",
        "      fname = image_id\n",
        "      dst = os.path.join('not_people_images', fname)\n",
        "      \n",
        "      # If cv2.COLOR_RGB2BGR is not used then the saved images appear blue.\n",
        "      cv2.imwrite(dst, image)\n",
        "      count = count + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yNceIlxdZem"
      },
      "outputs": [],
      "source": [
        "os.mkdir('dataset')\n",
        "shutil.move('/content/not_people_images','dataset/')\n",
        "shutil.move('/content/people_images','dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dKN2FFqVRGP"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Convert and store people dataframe to csv\n",
        "people.to_csv('people.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkzgxhD48klI"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "#!zip -r /content/people.zip /content/people_images\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/people.zip\")\n",
        "\n",
        "#!zip -r /content/not_people.zip /content/not_people_images\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/not_people.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AznKDe19XFn"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndnbALcXEbun"
      },
      "outputs": [],
      "source": [
        "# IMPORT DATASET\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224, 224)\n",
        "directory = \"dataset/\"\n",
        "train_dataset = image_dataset_from_directory(directory,\n",
        "                                             shuffle=True,\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='training',\n",
        "                                             seed=42)\n",
        "test_dataset = image_dataset_from_directory(directory,\n",
        "                                             shuffle=True,\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='validation',\n",
        "                                             seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd5BFuq1XhdE"
      },
      "outputs": [],
      "source": [
        "class_names = train_dataset.class_names\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAH6aGhGFBGY"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUbVmB-6FEiX"
      },
      "outputs": [],
      "source": [
        "def data_augmenter():\n",
        "    '''\n",
        "    Create a Sequential model composed of 2 layers\n",
        "    Returns:\n",
        "        tf.keras.Sequential\n",
        "    '''\n",
        "    ### START CODE HERE\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip(\"horizontal\"))\n",
        "    data_augmentation.add(RandomRotation(0.2))\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return data_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuJBavb_Xroy"
      },
      "outputs": [],
      "source": [
        "data_augmentation = data_augmenter()\n",
        "data_augmentation = data_augmenter()\n",
        "for image, _ in train_dataset.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = image[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "        plt.imshow(augmented_image[0] / 255)\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toxq30oZXsSq"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOPbMlMZGOhb"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (224,224)\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.MobileNetV2(include_top=False,weights='imagenet')\n",
        "base_model.summary()\n",
        "type(base_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get imagenet labels\n",
        "# import tensorflow_datasets as tfds\n",
        "# labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "# imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
        "\n",
        "# Set data_dir to a read-only storage of .tar files\n",
        "# Set write_dir to a w/r storage\n",
        "# data_dir = 'datasets/imagenet/'\n",
        "# write_dir = 'psando/tf-imagenet-dirs'\n",
        "\n",
        "# Construct a tf.data.Dataset\n",
        "# download_config = tfds.download.DownloadConfig(\n",
        "#                       extract_dir=os.path.join(write_dir, 'extracted'),\n",
        "#                       manual_dir=data_dir\n",
        "#                   )\n",
        "# download_and_prepare_kwargs = {\n",
        "#     'download_dir': os.path.join(write_dir, 'downloaded'),\n",
        "#     'download_config': download_config,\n",
        "# }\n",
        "# ds = tfds.load('imagenet2012_subset', \n",
        "#                data_dir=os.path.join(write_dir, 'data'),         \n",
        "#                split='train', \n",
        "#                shuffle_files=False, \n",
        "#                download=True, \n",
        "#                as_supervised=True,\n",
        "#                download_and_prepare_kwargs=download_and_prepare_kwargs)\n",
        "\n",
        "# Set data_dir to a read-only storage of .tar files\n",
        "# Set write_dir to a w/r storage\n",
        "# data_dir = '/content/image net dataset'\n",
        "# write_dir = 'psando/tf-imagenet-dirs'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "1Jkn4G5Dab3U",
        "outputId": "52b93db8-5b61-47ac-bf8b-57fd941d7053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 254.22 KiB (download: 254.22 KiB, generated: 7.61 GiB, total: 7.61 GiB) to psando/tf-imagenet-dirs/data/imagenet2012_subset/1pct/5.0.0...\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4e34ffdb5ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                download_and_prepare_kwargs=download_and_prepare_kwargs)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Set data_dir to a read-only storage of .tar files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    325\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    481\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    482\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m               \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m           )\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0moptional_pipeline_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m       split_generators = self._split_generators(  # pylint: disable=unexpected-keyword-arg\n\u001b[0;32m-> 1185\u001b[0;31m           dl_manager, **optional_pipeline_kwargs)\n\u001b[0m\u001b[1;32m   1186\u001b[0m       \u001b[0;31m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m       \u001b[0;31m# https://github.com/tensorflow/datasets/issues/2537\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/image_classification/imagenet2012_subset.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Import ImageNet here to avoid circular dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ILSVRC2012_img_train.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mval_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ILSVRC2012_img_val.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m       \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mmanual_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manual_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manual_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m       raise AssertionError(\n\u001b[0;32m--> 649\u001b[0;31m           \u001b[0;34mf'Manual directory {self._manual_dir} does not exist or is empty. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m           \u001b[0;34m'Create it and download/extract dataset artifacts in there using '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m           f'instructions:\\n{self._manual_dir_instructions}')\n",
            "\u001b[0;31mAssertionError\u001b[0m: Manual directory datasets/imagenet does not exist or is empty. Create it and download/extract dataset artifacts in there using instructions:\nmanual_dir should contain two files: ILSVRC2012_img_train.tar and\nILSVRC2012_img_val.tar.\nYou need to register on https://image-net.org/download-images in order\nto get the link to download the dataset."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V-NuKUJgbKwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OHzi42c2O43"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = 6000\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.30,\n",
        "                                                               final_sparsity=0.90,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(base_model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh1l182NPnFu"
      },
      "outputs": [],
      "source": [
        "# logdir = tempfile.mkdtemp()\n",
        "\n",
        "# callbacks = [\n",
        "#   tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "#   tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "# ]\n",
        "\n",
        "# model_for_pruning.fit(train_dataset,\n",
        "#                   batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "#                   callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aac2mhk69u7Z"
      },
      "outputs": [],
      "source": [
        "# # Create a base model\n",
        "# #base_model = tf.keras.applications.MobileNetV2(include_top=False,weights='imagenet')\n",
        "# #base_model.load_weights('imagenet') # optional but recommended for model accuracy\n",
        "\n",
        "# # Helper function uses `prune_low_magnitude` to make only the \n",
        "# # Dense layers train with pruning.\n",
        "# def apply_pruning_to_dense(layer):\n",
        "#   if isinstance(layer, tf.keras.layers.Dense):\n",
        "#     return tfmot.sparsity.keras.prune_low_magnitude(layer)\n",
        "#   return layer\n",
        "\n",
        "# # Use `tf.keras.models.clone_model` to apply `apply_pruning_to_dense` \n",
        "# # to the layers of the model.\n",
        "# model_for_pruning = tf.keras.models.clone_model(\n",
        "#     base_model,\n",
        "#     clone_function=apply_pruning_to_dense,\n",
        "# )\n",
        "\n",
        "# model_for_pruning.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CftL8YTbGuer"
      },
      "outputs": [],
      "source": [
        "image_batch, label_batch = next(iter(train_dataset))\n",
        "\n",
        "#Resize 32x32 to 224x224\n",
        "tf.image.resize(\n",
        "    image_batch,\n",
        "    (224,224),\n",
        "    preserve_aspect_ratio=True)\n",
        "# feature_batch = base_model(image_batch)\n",
        "# print(feature_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6La1ibCtGvHz"
      },
      "outputs": [],
      "source": [
        "def people_model(base_model,image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
        "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
        "    Arguments:\n",
        "        image_shape -- Image width and height\n",
        "        data_augmentation -- data augmentation function\n",
        "    Returns:\n",
        "    Returns:\n",
        "        tf.keras.model\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    input_shape = image_shape + (3,)\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # base_model = tf.keras.applications.MobileNetV2(input_shape= input_shape,\n",
        "    #                                                include_top=False, # <== Important!!!!\n",
        "    #                                                weights='imagenet') # From imageNet\n",
        "    \n",
        "    # freeze the base model by making it non trainable\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # create the input layer (Same as the imageNetv2 input size)\n",
        "    inputs = tf.keras.Input(shape=input_shape) \n",
        "    \n",
        "    # apply data augmentation to the inputs\n",
        "    x = data_augmentation(inputs)\n",
        "    \n",
        "    # data preprocessing using the same weights the model was trained on\n",
        "    x = preprocess_input(x) \n",
        "    \n",
        "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
        "    x = base_model(x, training=False) \n",
        "    \n",
        "    # add the new Binary classification layers\n",
        "    # use global avg pooling to summarize the info in each channel\n",
        "    x = tfl.GlobalAveragePooling2D()(x)  \n",
        "    # include dropout with probability of 0.2 to avoid overfitting\n",
        "    x = tfl.Dropout(0.2)(x)\n",
        "        \n",
        "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
        "    outputs = tfl.Dense(1)(x) \n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV8CeWlvHAAR"
      },
      "outputs": [],
      "source": [
        "model2 = people_model(base_model,IMG_SIZE, data_augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8ji09yoHImo"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.003\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV1zy8x39Ue9"
      },
      "outputs": [],
      "source": [
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WNb4wtJHKzP"
      },
      "outputs": [],
      "source": [
        "initial_epochs = 10\n",
        "history = model2.fit(train_dataset, validation_data=test_dataset, epochs=initial_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MnHZkdqHNOs"
      },
      "outputs": [],
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhY7xFzsY9NX"
      },
      "outputs": [],
      "source": [
        "#base_model = model2.layers[4]\n",
        "#base_model.trainable = True\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "# print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "#fine_tune_at = 120\n",
        "\n",
        "### START CODE HERE\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "# for layer in base_model.layers[:fine_tune_at]:\n",
        "#     layer.trainable = True\n",
        "    \n",
        "# Define a BinaryCrossentropy loss function. Use from_logits=True\n",
        "#loss_function=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.1*base_learning_rate)\n",
        "# Use accuracy as evaluation metric\n",
        "#metrics=['accuracy']\n",
        "\n",
        "### END CODE HERE\n",
        "\n",
        "#model2.compile(loss=loss_function,\n",
        "#              optimizer = optimizer,\n",
        " #             metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCwlyk5flnEq"
      },
      "outputs": [],
      "source": [
        "#fine_tune_epochs = 5\n",
        "#total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "#history_fine = model2.fit(train_dataset,\n",
        "#                         epochs=total_epochs,\n",
        "#                         initial_epoch=history.epoch[-1],\n",
        "#                         validation_data=validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek3n5XPPlpY7"
      },
      "outputs": [],
      "source": [
        "#acc += history_fine.history['accuracy']\n",
        "#val_acc += history_fine.history['val_accuracy']\n",
        "\n",
        "#loss += history_fine.history['loss']\n",
        "#val_loss += history_fine.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN4tgzHfls1T"
      },
      "outputs": [],
      "source": [
        "#plt.figure(figsize=(8, 8))\n",
        "#plt.subplot(2, 1, 1)\n",
        "#plt.plot(acc, label='Training Accuracy')\n",
        "#plt.plot(val_acc, label='Validation Accuracy')\n",
        "#plt.ylim([0, 1])\n",
        "#plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "#          plt.ylim(), label='Start Fine Tuning')\n",
        "#plt.legend(loc='lower right')\n",
        "#plt.title('Training and Validation Accuracy')\n",
        "\n",
        "#plt.subplot(2, 1, 2)\n",
        "#plt.plot(loss, label='Training Loss')\n",
        "#plt.plot(val_loss, label='Validation Loss')\n",
        "#plt.ylim([0, 1.0])\n",
        "#plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "#         plt.ylim(), label='Start Fine Tuning')\n",
        "#plt.legend(loc='upper right')\n",
        "#plt.title('Training and Validation Loss')\n",
        "#plt.xlabel('epoch')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtbI5WLG0l_3"
      },
      "outputs": [],
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model_for_export, \"/content/Saved_Model\", include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8MUytpxR4Jh"
      },
      "outputs": [],
      "source": [
        "# converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "# pruned_tflite_model = converter.convert()\n",
        "\n",
        "# _, tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "# with open(\"/content/tflite_file\", 'wb') as f:\n",
        "#   f.write(pruned_tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuGrZfV5SC9D"
      },
      "outputs": [],
      "source": [
        "#def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        " # import os\n",
        " # import zipfile\n",
        "\n",
        " # _, zipped_file = tempfile.mkstemp('.zip')\n",
        " # with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  #  f.write(file)\n",
        "\n",
        " # return os.path.getsize(zipped_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH_LvkaESHDa"
      },
      "outputs": [],
      "source": [
        "#print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "#print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS42-to9tPuQ"
      },
      "outputs": [],
      "source": [
        "# def representative_dataset():\n",
        "#     for _ in range(100):\n",
        "#       data = np.random.rand(1, 244, 244, 3)\n",
        "#       yield [data.astype(np.float32)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVFSZoVi4aKs"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, quantized_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(\"/content/Saved_Model.tflite\", 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_tflite_file)\n",
        "print(\"Size of tflite model \",sys.getsizeof(quantized_and_pruned_tflite_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y25yiyaASedQ"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "# converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "# quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "# _, quantized_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "# with open(\"/content/quantized_tflite_file\", 'wb') as f:\n",
        "#   f.write(quantized_and_pruned_tflite_model)\n",
        "# print('Saved quantized and pruned TFLite model to:', quantized_tflite_file)\n",
        "# print(\"Size of tflite model \",sys.getsizeof(quantized_and_pruned_tflite_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9beNR2-TIAb"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get -qq install xxd\n",
        "!xxd -i \"/content/Saved_Model.tflite\" > model_data.cc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}